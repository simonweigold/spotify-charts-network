---
title: "network_analysis"
author: "Simon Weigold"
date: "2023-06-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r setup, include=FALSE, eval = F}
library(here)
source(here::here("scripts", "preparation.R"))
```

# Exploratory Data Analysis
Frequency distribution of streams
```{r}
freq_streams <- global %>% 
  ggplot(aes(x = streams)) +
  geom_freqpoly(bins = length(streams), col = "royalblue4") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )

ggsave(freq_streams,
       filename = "freq_streams.png",
       path = here::here("imgs"),
       device = "png",
       width = 6, height = 4, units = "in",
       dpi = 600
       )

freq_streams
```

Frequency Distribution of Popularity
```{r}
freq_popularity <- metrics %>% 
  ggplot(aes(x = popularity)) +
  geom_freqpoly(binwidth = length(popularity), col = "royalblue4", na.rm = T) +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )

ggsave(freq_popularity,
       filename = "freq_popularity.png",
       path = here::here("imgs"),
       device = "png",
       width = 6, height = 4, units = "in",
       dpi = 600
       )

freq_popularity
```

Most occuring artists and wordcloud
```{r}
dplyr::count(df, artist, sort = T) %>% head(25)

wordcloud_artists <- wordcloud2::wordcloud2(data = count(df, artist), size = 0.35, color = "random-dark")
wordcloud_artists
#saveWidget(wordcloud_artists, here::here("imgs", "wordcloud-artists.html"), selfcontained = F)
#webshot(here::here("imgs", "wordcloud-artists.html"), here::here("imgs", "wordcloud-artists.png"), delay = 20, vwidth = 960, vheight = 960)
```

Most occuring songs and wordcloud
```{r}
dplyr::count(df, title, sort = T) %>% head(25)

wordcloud_songs <- wordcloud2::wordcloud2(data = count(df, title), size = 1, color = "random-dark")
wordcloud_songs
#saveWidget(wordcloud_songs, here::here("imgs", "wordcloud-songs.html"), selfcontained = F)
#webshot(here::here("imgs", "wordcloud-songs.html"), here::here("imgs", "wordcloud-songs.png"), delay =5, vwidth = 480, vheight=480)
```

```{r}
dep_var = inner_join(dep_var, popularity, by = "artist")
dep_var = inner_join(dep_var, genres, by = "artist")
dep_var = dep_var %>% select(-genre3)
dep_var = dep_var[order(-dep_var[,'Freq']),]
dep_var %>% head(25)
```

```{r}
dep_var = dep_var[order(-dep_var[,'streams']),]
dep_var %>% head(25)
```

```{r}
cor(dep_var$Freq, dep_var$streams)
cor(dep_var$Freq, dep_var$popularity, use = "na.or.complete")
```

```{r}
dep_var = dep_var[order(-dep_var[,'output']),]
dep_var %>% head(25)
```

Distribution of Genres
```{r}
genre_freq <- as.data.frame(table(genres$Genre))
genre_freq[order(-genre_freq[,'Freq']),] %>% head(25)
```

Distribution of meta Genres
```{r}
genre_freq <- as.data.frame(table(genres$genre2))
genre_freq[order(-genre_freq[,'Freq']),] %>% head(25)
```

```{r}
# Create plot_data
plot_data <- metrics_minmax %>% 
  tidyr::pivot_longer(cols = -c(artist, Genre, genre2, genre3),
                      names_to = "key", values_to = "value")
# Visualise relation distribution between variables and quality
plot_data %>% 
  ggplot2::ggplot(aes(x = value), na.rm = T) + 
  ggplot2::geom_density(alpha = 0.6) +
  ggplot2::facet_wrap(~ key, scales = "free")
# Visualise relation between variables and quality in boxplots
plot_data %>%
  ggplot2::ggplot(aes(y = value), na.rm = T)+
  ggplot2::geom_boxplot(alpha =0.6)+
  ggplot2::facet_wrap(~key, scales ="free")+
  ggplot2::theme(axis.title.x =element_blank(),
                 axis.text.x =element_blank(),
                 axis.ticks.x =element_blank())
```

Exploratory network analysis \n
```{r}
gsize(largest_subgraph) #size
edge_density(largest_subgraph) #density
count_components(largest_subgraph) #components
diameter(largest_subgraph, directed = F) #diameter
transitivity(largest_subgraph) #clustering
```


# Hypothesis 1
## Centrality measures
As a first step, the centrality measures are calculated.
```{r}
# see preparation
```

## Diagnostics of regression model
```{r}
regression <- lm(popularity ~ degree + betweenness + closeness + eigenvector,
                 data = metrics_minmax, na.action = na.exclude)
```

Diagnostics
1. Linearity
```{r}
avPlots(regression)
```
2. No multicollinearity
```{r}
metrics_minmax %>%
  dplyr::select(c(popularity, degree, betweenness, closeness, eigenvector)) %>% 
  stats::cor(use = "na.or.complete") %>% 
  corrplot::corrplot.mixed(upper = "circle",
                 lower = "number",
                 tl.pos = "lt",
                 tl.col = "black",
                 lower.col = "black",
                 number.cex = 1)

CT <- cbind(metrics_minmax$degree, metrics_minmax$betweenness, metrics_minmax$closeness, metrics_minmax$eigenvector)
rcorr(CT)

vif(regression)
```
3. Homoskedasticity of the residuals
```{r}
plot(regression, 1)

bptest(regression)
```
4. Normality of the residuals
```{r}
hist(regression$residuals, breaks= 30, freq = F,
     main = "Distribution of residuals",
     xlab = "Residuals")
curve(dnorm(x, mean(regression$residuals), sd(regression$residuals)), 
      col = "red",
      add = T)
plot(regression, 2)
```

5. No influential points
```{r}
plot(regression, 4)
```

## Penalized model
Because there is a significant multicollinearity, which can be explained theoretically, a penalized linear regression model with an elastic net is applied.
```{r}
no_nas <- na.omit(metrics_minmax)
# Separate the independent variables and the dependent variable
plm_x <- as.matrix(no_nas[, c("degree", "betweenness", "eigenvector", "closeness")])
plm_y <- no_nas$popularity

# Perform penalized linear regression using glmnet
penalized_regression <- glmnet(plm_x, plm_y)

# Set up lambda values for tuning
lambda_seq <- 10^seq(-2, 2, by = 0.5)

# Perform cross-validation to tune the lambda parameter
cv_result <- cv.glmnet(plm_x, plm_y, lambda = lambda_seq)

# Find the optimal lambda value
optimal_lambda <- cv_result$lambda.min

# Fit the penalized regression model with the optimal lambda
penalized_regression_optimal <- glmnet(plm_x, plm_y, lambda = optimal_lambda)

# Predict the response variable using the optimal model
plm_y_pred <- predict(penalized_regression_optimal, newx = plm_x)

# Calculate the mean squared error (MSE)
mse <- mean((plm_y - plm_y_pred)^2)

# Calculate deviance explained
null_deviance <- sum((plm_y - mean(plm_y))^2)
deviance_explained <- 1 - (sum((plm_y - plm_y_pred)^2) / null_deviance)
```

"When a penalized regression model, such as ridge regression or lasso regression, returns a coefficient with no value (i.e., NA or zero), it typically indicates that the corresponding predictor variable has been effectively excluded from the model.

In penalized regression, the regularization penalty is applied to the model to shrink the coefficients towards zero. This penalty encourages sparsity in the model, meaning that some coefficients may be set exactly to zero. This is particularly the case in lasso regression, where the L1 penalty promotes exact zeroing of coefficients, resulting in variable selection.

When a coefficient has no value in the output, it means that the corresponding predictor variable does not contribute to the model's predictions or is deemed less important by the regularization process. In other words, the model has effectively removed or excluded that variable from the final model.

It's important to note that the exclusion of a variable does not necessarily mean it is irrelevant or unimportant in the broader context. The penalized regression model is making a trade-off between simplicity and prediction accuracy, and the excluded variables may not significantly improve the model's performance compared to the remaining variables.

If you believe that a particular variable is important and should be included in the model, you may consider adjusting the regularization parameter (lambda) or trying alternative regression techniques that do not enforce sparsity, such as ordinary least squares (OLS) regression" (ChatGPT, 24.06.2023).
```{r}
# Print the coefficients and R-squared of the optimal model
print(coef(penalized_regression_optimal))
print(paste("MSE:", mse))
print(paste("Deviance Explained:", deviance_explained))
```

```{r}
plot(penalized_regression, xvar = "lambda", label = TRUE)


# Plot the predicted values and the original values
plot(plm_y, type = "o", col = "blue", ylim = range(c(plm_y, plm_y_pred)), 
     xlab = "Observation", ylab = "Value", main = "Predicted vs Original Values")
lines(plm_y_pred, type = "o", col = "red")
legend("topright", legend = c("Original", "Predicted"), 
       col = c("blue", "red"), pch = c(1, 1))


# Plot
ggplot2::ggplot() +
  ggplot2::geom_point(data = as.data.frame(cbind(plm_y, plm_y_pred)),
                mapping = aes(x = plm_y, y = s0, col = "royalblue4")) +
  ggplot2::geom_smooth(method = "lm", se = F, col = "red")
#plot(metrics_minmax$streams, plm_y_pred)
#abline(metrics_minmax$streams, plm_y_pred)
```



# Hypothesis 2

```{r}
cor(metrics$degree, metrics$gs_degree)
cor(metrics$betweenness, metrics$gs_betweenness)
cor(metrics$closeness, metrics$gs_closeness, use = "na.or.complete")
cor(metrics$eigenvector, metrics$gs_eigenvector)
```

```{r}
# ANOVA streams
#stats::oneway.test(AV ~ Gruppe, data = Daten)
stats::oneway.test(popularity ~ genre2, data = metrics)
```
```{r}
# Separate the independent variables and the dependent variable
gs_plm_x <- as.matrix(no_nas[, c("gs_degree", "gs_betweenness", "gs_eigenvector", "gs_closeness")])
gs_plm_y <- no_nas$popularity

# Perform penalized linear regression using glmnet
gs_penalized_regression <- glmnet(gs_plm_x, gs_plm_y)

# Set up lambda values for tuning
gs_lambda_seq <- 10^seq(-2, 2, by = 0.5)

# Perform cross-validation to tune the lambda parameter
gs_cv_result <- cv.glmnet(gs_plm_x, gs_plm_y, lambda = gs_lambda_seq)

# Find the optimal lambda value
gs_optimal_lambda <- gs_cv_result$lambda.min

# Fit the penalized regression model with the optimal lambda
gs_penalized_regression_optimal <- glmnet(gs_plm_x, gs_plm_y, lambda = gs_optimal_lambda)

# Predict the response variable using the optimal model
gs_plm_y_pred <- predict(gs_penalized_regression_optimal, newx = gs_plm_x)

# Calculate the mean squared error (MSE)
gs_mse <- mean((gs_plm_y - gs_plm_y_pred)^2)

# Calculate deviance explained
gs_null_deviance <- sum((gs_plm_y - mean(gs_plm_y))^2)
gs_deviance_explained <- 1 - (sum((gs_plm_y - gs_plm_y_pred)^2) / gs_null_deviance)
```

```{r}
# Print the coefficients and R-squared of the optimal model
print(coef(gs_penalized_regression_optimal))
print(paste("MSE:", gs_mse))
print(paste("Deviance Explained:", gs_deviance_explained))
```

```{r}
plot(gs_penalized_regression, xvar = "lambda", label = TRUE)


# Plot the predicted values and the original values
plot(gs_plm_y, type = "o", col = "blue", ylim = range(c(gs_plm_y, gs_plm_y_pred)), 
     xlab = "Observation", ylab = "Value", main = "Predicted vs Original Values")
lines(gs_plm_y_pred, type = "o", col = "red")
legend("topright", legend = c("Original", "Predicted"), 
       col = c("blue", "red"), pch = c(1, 1))


# Plot
ggplot2::ggplot(data = as.data.frame(cbind(gs_plm_y, gs_plm_y_pred)),
                mapping = aes(x = gs_plm_y, y = s0, col = s0)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm", se = F, col = "red")
#plot(metrics_minmax$streams, plm_y_pred)
#abline(metrics_minmax$streams, plm_y_pred)
```