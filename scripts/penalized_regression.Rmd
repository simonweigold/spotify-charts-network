---
title: "penalized_regression"
author: "Simon Weigold"
date: "2023-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup, include=FALSE, eval = F}
library(here)
source(here::here("scripts", "preparation.R"))
```


## Penalized model
Because there is a significant multicollinearity, which can be explained theoretically, a penalized linear regression model with an elastic net is applied.

### according to [David Dalpiaz](https://daviddalpiaz.github.io/r4sl/elastic-net.html)

```{r}
# set up data
no_nas <- na.omit(metrics_minmax %>% select(-c(closeness, gs_closeness))) # remove NAs and closeness
```

```{r}
set.seed(42)
# set up 10 fold CV
cv_10 = trainControl(method = "cv", number = 10)
# fit the elastic net
pop_elnet = train(
  popularity ~ degree + betweenness + eigenvector, data = no_nas,
  method = "glmnet",
  trControl = cv_10,
  tuneLength = 10
)
# retrieve best parameters
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(pop_elnet)
```



```{r}
# visualise elnet
pop_elnet_results <- as.data.frame(pop_elnet$results)
# lambda_graphs = list()
# for (i in length(unique(pop_elnet_results$alpha))) {
#   temp <- pop_elnet_results %>% filter(alpha == unique(pop_elnet_results$alpha)[i])
#   temp %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
#   #label = str_c("alpha", " = ", unique(pop_elnet_results$alpha)[i])
# }
# pop_elnet_results %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
# temp <- pop_elnet_results %>% filter(alpha == unique(pop_elnet_results$alpha)[1])
# temp %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
# plot(pop_elnet_results$lambda, pop_elnet_results$RMSE)

# lambda graph
pop_elnet_results %>% ggplot(aes(x = alpha, y = RMSE, col = lambda)) + geom_point() +
    theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )
pop_elnet_results %>% ggplot(aes(x = lambda, y = RMSE, col = alpha)) + geom_point() +
    theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )
```



```{r}
X = model.matrix(popularity ~ degree + betweenness + eigenvector, data = no_nas)[, -1]
y = no_nas$popularity

fit_enr_cv = cv.glmnet(X, y, alpha = 0.1)
sqrt(fit_enr_cv$cvm[fit_enr_cv$lambda == fit_enr_cv$lambda.min]) # CV-RMSE minimum

```

```{r}
sum(coef(fit_enr_cv) != 0)
sum(coef(fit_enr_cv) == 0)

coef(fit_enr_cv)
fit_enr_cv$index

# explore predictions
enr_preds <- predict(fit_enr_cv, newx = X)
enr_preds <- cbind(enr_preds, y)
enr_preds <- as.data.frame(enr_preds)
colnames(enr_preds)[1] <- "preds"
colnames(enr_preds)[2] <- "pop"
enr_preds %>%
  ggplot(aes(x = pop, y = preds)) +
  geom_point() +
  geom_abline(col = "red", size = 1)

# Calculate the mean squared error (MSE)
mse <- mean((y - enr_preds$preds)^2)

# Calculate deviance explained
null_deviance <- sum((y - mean(y))^2)
deviance_explained <- 1 - (sum((y - enr_preds$preds)^2) / null_deviance)
```


```{r}
# Print the coefficients and R-squared of the optimal model
print(coef(fit_enr_cv))
print(paste("MSE:", mse))
print(paste("Deviance Explained:", deviance_explained))
```

```{r}
plot(fit_enr_cv, xvar = "lambda")
plot(fit_enr_cv, xvar = "alpha")


# Plot the predicted values and the original values
plot(y, type = "o", col = "blue", ylim = range(c(y, regular_reg)), 
     xlab = "Observation", ylab = "Value", main = "Predicted vs Original Values")
lines(enr_preds$preds, type = "o", col = "red")
legend("topright", legend = c("Original", "Predicted"), 
       col = c("blue", "red"), pch = c(1, 1))


# Plot
ggplot2::ggplot() +
  ggplot2::geom_point((data = enr_preds),
                mapping = aes(x = pop, y = preds, col = "royalblue4")) +
  ggplot2::geom_smooth(method = "lm", se = F, col = "red")
#plot(metrics_minmax$streams, plm_y_pred)
#abline(metrics_minmax$streams, plm_y_pred)
```



# Hypothesis 2

```{r}
cor(metrics$degree, metrics$gs_degree)
cor(metrics$betweenness, metrics$gs_betweenness)
cor(metrics$closeness, metrics$gs_closeness, use = "na.or.complete")
cor(metrics$eigenvector, metrics$gs_eigenvector)
```

```{r}
# ANOVA streams
#stats::oneway.test(AV ~ Gruppe, data = Daten)
stats::oneway.test(popularity ~ genre2, data = metrics)
```







