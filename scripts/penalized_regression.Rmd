---
title: "penalized_regression"
author: "Simon Weigold"
date: "2023-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r setup, include=FALSE, eval = F}
library(here)
source(here::here("scripts", "preparation.R"))
```


## Penalized model
Because there is a significant multicollinearity, which can be explained theoretically, a penalized linear regression model with an elastic net is applied.

### according to [David Dalpiaz](https://daviddalpiaz.github.io/r4sl/elastic-net.html)
```{r}
set.seed(42)
# set up 10 fold CV
cv_10 = trainControl(method = "cv", number = 10)
# fit the elastic net
pop_elnet = train(
  popularity ~ degree + betweenness + eigenvector, data = no_nas,
  method = "glmnet",
  trControl = cv_10,
  tuneLength = 10
)
# retrieve best parameters
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(pop_elnet)
```



```{r}
# visualise elnet
pop_elnet_results <- as.data.frame(pop_elnet$results)
# lambda_graphs = list()
# for (i in length(unique(pop_elnet_results$alpha))) {
#   temp <- pop_elnet_results %>% filter(alpha == unique(pop_elnet_results$alpha)[i])
#   temp %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
#   #label = str_c("alpha", " = ", unique(pop_elnet_results$alpha)[i])
# }
# pop_elnet_results %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
# temp <- pop_elnet_results %>% filter(alpha == unique(pop_elnet_results$alpha)[1])
# temp %>% ggplot(aes(x = lambda, y = RMSE)) + geom_line()
# plot(pop_elnet_results$lambda, pop_elnet_results$RMSE)

# lambda graph
pop_elnet_results %>% ggplot(aes(x = alpha, y = RMSE, col = lambda)) + geom_point() +
    theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )
pop_elnet_results %>% ggplot(aes(x = lambda, y = RMSE, col = alpha)) + geom_point() +
    theme(axis.text = element_text(color="black", size=12, family="serif"),
        axis.text.x = element_text(color="black", size=12, family="serif"),
        axis.ticks.x = element_line(),
        axis.ticks.y = element_line(),
        axis.line.x = element_line(size=0.5, color="grey"),
        axis.line.y = element_line(size=0.5, color="grey"),
        panel.grid = element_line(color = "honeydew2",
                                  size = 0.5,
                                  linetype = 1),
        panel.background = element_rect(fill="white"),
        plot.margin = margin(10,10,10,10),
        plot.title = element_text(color="black", size=16, family="serif"),
        plot.subtitle = element_text(color="grey26", size=14, family="serif"),
        legend.text = element_text(color="black", size=12, family="serif"),
        text = element_text(color="black", size=14, family="serif")
  )
```



```{r}
X = model.matrix(popularity ~ degree + betweenness + eigenvector, data = no_nas)[, -1]
y = no_nas$popularity

fit_enr_cv = cv.glmnet(X, y, alpha = 0.1)
sqrt(fit_enr_cv$cvm[fit_enr_cv$lambda == fit_enr_cv$lambda.min]) # CV-RMSE minimum

```

```{r}
sum(coef(fit_enr_cv) != 0)
sum(coef(fit_enr_cv) == 0)

coef(fit_enr_cv)
fit_enr_cv$index

# explore predictions
enr_preds <- predict(fit_enr_cv, newx = X)
enr_preds <- cbind(enr_preds, y)
enr_preds <- as.data.frame(enr_preds)
colnames(enr_preds)[1] <- "preds"
colnames(enr_preds)[2] <- "pop"
enr_preds %>%
  ggplot(aes(x = pop, y = preds)) +
  geom_point() +
  geom_abline(col = "red", size = 1)
```





### according to ChatGPT
```{r}
no_nas <- na.omit(metrics_minmax %>% select(-c(closeness, gs_closeness)))
# Separate the independent variables and the dependent variable
plm_x <- as.matrix(no_nas[, c("degree", "betweenness", "eigenvector", "closeness")])
plm_y <- no_nas$popularity

# Perform penalized linear regression using glmnet
penalized_regression <- glmnet(plm_x, plm_y)

# Set up lambda values for tuning
lambda_seq <- 10^seq(-2, 2, by = 0.5)

# Perform cross-validation to tune the lambda parameter
cv_result <- cv.glmnet(plm_x, plm_y, lambda = lambda_seq)

# Find the optimal lambda value
optimal_lambda <- cv_result$lambda.min

# Fit the penalized regression model with the optimal lambda
penalized_regression_optimal <- glmnet(plm_x, plm_y, lambda = optimal_lambda)

# Predict the response variable using the optimal model
plm_y_pred <- predict(penalized_regression_optimal, newx = plm_x)

# Calculate the mean squared error (MSE)
mse <- mean((plm_y - plm_y_pred)^2)

# Calculate deviance explained
null_deviance <- sum((plm_y - mean(plm_y))^2)
deviance_explained <- 1 - (sum((plm_y - plm_y_pred)^2) / null_deviance)
```

"When a penalized regression model, such as ridge regression or lasso regression, returns a coefficient with no value (i.e., NA or zero), it typically indicates that the corresponding predictor variable has been effectively excluded from the model.

In penalized regression, the regularization penalty is applied to the model to shrink the coefficients towards zero. This penalty encourages sparsity in the model, meaning that some coefficients may be set exactly to zero. This is particularly the case in lasso regression, where the L1 penalty promotes exact zeroing of coefficients, resulting in variable selection.

When a coefficient has no value in the output, it means that the corresponding predictor variable does not contribute to the model's predictions or is deemed less important by the regularization process. In other words, the model has effectively removed or excluded that variable from the final model.

It's important to note that the exclusion of a variable does not necessarily mean it is irrelevant or unimportant in the broader context. The penalized regression model is making a trade-off between simplicity and prediction accuracy, and the excluded variables may not significantly improve the model's performance compared to the remaining variables.

If you believe that a particular variable is important and should be included in the model, you may consider adjusting the regularization parameter (lambda) or trying alternative regression techniques that do not enforce sparsity, such as ordinary least squares (OLS) regression" (ChatGPT, 24.06.2023).
```{r}
# Print the coefficients and R-squared of the optimal model
print(coef(penalized_regression_optimal))
print(paste("MSE:", mse))
print(paste("Deviance Explained:", deviance_explained))
```

```{r}
plot(penalized_regression, xvar = "lambda", label = TRUE)


# Plot the predicted values and the original values
plot(plm_y, type = "o", col = "blue", ylim = range(c(plm_y, plm_y_pred)), 
     xlab = "Observation", ylab = "Value", main = "Predicted vs Original Values")
lines(plm_y_pred, type = "o", col = "red")
legend("topright", legend = c("Original", "Predicted"), 
       col = c("blue", "red"), pch = c(1, 1))


# Plot
ggplot2::ggplot() +
  ggplot2::geom_point(data = as.data.frame(cbind(plm_y, plm_y_pred)),
                mapping = aes(x = plm_y, y = s0, col = "royalblue4")) +
  ggplot2::geom_smooth(method = "lm", se = F, col = "red")
#plot(metrics_minmax$streams, plm_y_pred)
#abline(metrics_minmax$streams, plm_y_pred)
```



# Hypothesis 2

```{r}
cor(metrics$degree, metrics$gs_degree)
cor(metrics$betweenness, metrics$gs_betweenness)
cor(metrics$closeness, metrics$gs_closeness, use = "na.or.complete")
cor(metrics$eigenvector, metrics$gs_eigenvector)
```

```{r}
# ANOVA streams
#stats::oneway.test(AV ~ Gruppe, data = Daten)
stats::oneway.test(popularity ~ genre2, data = metrics)
```
```{r}
# Separate the independent variables and the dependent variable
gs_plm_x <- as.matrix(no_nas[, c("gs_degree", "gs_betweenness", "gs_eigenvector", "gs_closeness")])
gs_plm_y <- no_nas$popularity

# Perform penalized linear regression using glmnet
gs_penalized_regression <- glmnet(gs_plm_x, gs_plm_y)

# Set up lambda values for tuning
gs_lambda_seq <- 10^seq(-2, 2, by = 0.5)

# Perform cross-validation to tune the lambda parameter
gs_cv_result <- cv.glmnet(gs_plm_x, gs_plm_y, lambda = gs_lambda_seq)

# Find the optimal lambda value
gs_optimal_lambda <- gs_cv_result$lambda.min

# Fit the penalized regression model with the optimal lambda
gs_penalized_regression_optimal <- glmnet(gs_plm_x, gs_plm_y, lambda = gs_optimal_lambda)

# Predict the response variable using the optimal model
gs_plm_y_pred <- predict(gs_penalized_regression_optimal, newx = gs_plm_x)

# Calculate the mean squared error (MSE)
gs_mse <- mean((gs_plm_y - gs_plm_y_pred)^2)

# Calculate deviance explained
gs_null_deviance <- sum((gs_plm_y - mean(gs_plm_y))^2)
gs_deviance_explained <- 1 - (sum((gs_plm_y - gs_plm_y_pred)^2) / gs_null_deviance)
```

```{r}
# Print the coefficients and R-squared of the optimal model
print(coef(gs_penalized_regression_optimal))
print(paste("MSE:", gs_mse))
print(paste("Deviance Explained:", gs_deviance_explained))
```

```{r}
plot(gs_penalized_regression, xvar = "lambda", label = TRUE)


# Plot the predicted values and the original values
plot(gs_plm_y, type = "o", col = "blue", ylim = range(c(gs_plm_y, gs_plm_y_pred)), 
     xlab = "Observation", ylab = "Value", main = "Predicted vs Original Values")
lines(gs_plm_y_pred, type = "o", col = "red")
legend("topright", legend = c("Original", "Predicted"), 
       col = c("blue", "red"), pch = c(1, 1))


# Plot
ggplot2::ggplot(data = as.data.frame(cbind(gs_plm_y, gs_plm_y_pred)),
                mapping = aes(x = gs_plm_y, y = s0, col = s0)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = "lm", se = F, col = "red")
#plot(metrics_minmax$streams, plm_y_pred)
#abline(metrics_minmax$streams, plm_y_pred)
```








